{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data from data\\preprocessed\\sign_mnist_preprocessed.npz...\n",
      "Preprocessed data loaded successfully.\n",
      "Shapes: (27455, 28, 28, 1) (27455, 24) (7172, 28, 28, 1) (7172, 24)\n",
      "Classes: [ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed data\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define path to the preprocessed data file\n",
    "preprocessed_file_path = os.path.join(\"data\", \"preprocessed\", \"sign_mnist_preprocessed.npz\")\n",
    "\n",
    "# Load the preprocessed data\n",
    "print(f\"Loading preprocessed data from {preprocessed_file_path}...\")\n",
    "try:\n",
    "    loaded_data = np.load(preprocessed_file_path)\n",
    "    x_train = loaded_data['x_train']\n",
    "    y_train = loaded_data['y_train']\n",
    "    x_test = loaded_data['x_test']\n",
    "    y_test = loaded_data['y_test']\n",
    "    classes = loaded_data['classes']\n",
    "    print(\"Preprocessed data loaded successfully.\")\n",
    "    print(\"Shapes:\", x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "    print(\"Classes:\", classes)\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Preprocessed data file not found at {preprocessed_file_path}\")\n",
    "    print(\"Please run 'python preprocess_data.py' first to generate it.\")\n",
    "    raise SystemExit(\"Preprocessed data not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import random\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from array import array\n",
    "from os.path  import join\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# # Importing necessary library\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# # Define the number of splits for K-Fold Cross-Validation\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# # Lists to store performance metrics\n",
    "# accuracies = []\n",
    "# precisions = []\n",
    "# recalls = []\n",
    "# f1_scores = []\n",
    "\n",
    "# # Iterate through each fold\n",
    "# for train_index, val_index in kf.split(x_train, y_train):\n",
    "#     # Split the data into training and validation sets based on current fold\n",
    "#     x_fold_train, x_fold_val = x_train[train_index], x_train[val_index]\n",
    "#     y_fold_train, y_fold_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "#     # Reinitialize the model for each fold\n",
    "#     fold_model = Sequential()\n",
    "#     fold_model.add(Conv2D(75, (3,3), strides=1, padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "#     fold_model.add(BatchNormalization())\n",
    "#     fold_model.add(MaxPool2D((2,2), strides=2, padding='same'))\n",
    "#     fold_model.add(Conv2D(50, (3,3), strides=1, padding='same', activation='relu'))\n",
    "#     fold_model.add(Dropout(0.2))\n",
    "#     fold_model.add(BatchNormalization())\n",
    "#     fold_model.add(MaxPool2D((2,2), strides=2, padding='same'))\n",
    "#     fold_model.add(Conv2D(25, (3,3), strides=1, padding='same', activation='relu'))\n",
    "#     fold_model.add(BatchNormalization())\n",
    "#     fold_model.add(MaxPool2D((2,2), strides=2, padding='same'))\n",
    "#     fold_model.add(Flatten())\n",
    "#     fold_model.add(Dense(512, activation='relu'))\n",
    "#     fold_model.add(Dropout(0.3))\n",
    "#     fold_model.add(Dense(24, activation='softmax'))\n",
    "#     fold_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Importing necessary library\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# # Define the number of splits for K-Fold Cross-Validation\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# # Lists to store performance metrics\n",
    "# accuracies = []\n",
    "# precisions = []\n",
    "# recalls = []\n",
    "# f1_scores = []\n",
    "\n",
    "# # Iterate through each fold\n",
    "# for train_index, val_index in kf.split(x_train, y_train):\n",
    "#     # Split the data into training and validation sets based on current fold\n",
    "#     x_fold_train, x_fold_val = x_train[train_index], x_train[val_index]\n",
    "#     y_fold_train, y_fold_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "#     # Reinitialize the model for each fold\n",
    "#     fold_model = Sequential()\n",
    "#     fold_model.add(Conv2D(75, (3,3), strides=1, padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "#     fold_model.add(BatchNormalization())\n",
    "#     fold_model.add(MaxPool2D((2,2), strides=2, padding='same'))\n",
    "#     fold_model.add(Conv2D(50, (3,3), strides=1, padding='same', activation='relu'))\n",
    "#     fold_model.add(Dropout(0.2))\n",
    "#     fold_model.add(BatchNormalization())\n",
    "#     fold_model.add(MaxPool2D((2,2), strides=2, padding='same'))\n",
    "#     fold_model.add(Conv2D(25, (3,3), strides=1, padding='same', activation='relu'))\n",
    "#     fold_model.add(BatchNormalization())\n",
    "#     fold_model.add(MaxPool2D((2,2), strides=2, padding='same'))\n",
    "#     fold_model.add(Flatten())\n",
    "#     fold_model.add(Dense(512, activation='relu'))\n",
    "#     fold_model.add(Dropout(0.3))\n",
    "#     fold_model.add(Dense(24, activation='softmax'))\n",
    "#     fold_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ImageDataGenerator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m     \u001b[38;5;66;03m# Define data augmentation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m datagen = \u001b[43mImageDataGenerator\u001b[49m(\n\u001b[32m      3\u001b[39m     rotation_range=\u001b[32m10\u001b[39m,\n\u001b[32m      4\u001b[39m     zoom_range=\u001b[32m0.1\u001b[39m,\n\u001b[32m      5\u001b[39m     width_shift_range=\u001b[32m0.1\u001b[39m,\n\u001b[32m      6\u001b[39m     height_shift_range=\u001b[32m0.1\u001b[39m,\n\u001b[32m      7\u001b[39m     horizontal_flip=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m      8\u001b[39m     vertical_flip=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m      9\u001b[39m     fill_mode=\u001b[33m'\u001b[39m\u001b[33mnearest\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Train the model on the training fold data\u001b[39;00m\n\u001b[32m     13\u001b[39m fold_model.fit(datagen.flow(x_fold_train, y_fold_train, batch_size=\u001b[32m128\u001b[39m),\n\u001b[32m     14\u001b[39m                epochs=\u001b[32m10\u001b[39m,\n\u001b[32m     15\u001b[39m                validation_data=(x_fold_val, y_fold_val),\n\u001b[32m     16\u001b[39m                callbacks=[learning_rate_reduction])\n",
      "\u001b[31mNameError\u001b[39m: name 'ImageDataGenerator' is not defined"
     ]
    }
   ],
   "source": [
    "#     # Define data augmentation\n",
    "# datagen = ImageDataGenerator(\n",
    "#     rotation_range=10,\n",
    "#     zoom_range=0.1,\n",
    "#     width_shift_range=0.1,\n",
    "#     height_shift_range=0.1,\n",
    "#     horizontal_flip=False,\n",
    "#     vertical_flip=False,\n",
    "#     fill_mode='nearest'\n",
    "# )\n",
    "\n",
    "# # Train the model on the training fold data\n",
    "# fold_model.fit(datagen.flow(x_fold_train, y_fold_train, batch_size=128),\n",
    "#                epochs=10,\n",
    "#                validation_data=(x_fold_val, y_fold_val),\n",
    "#                callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # Evaluate the model on the validation set for the current fold\n",
    "#     predictions = fold_model.predict(x_fold_val)\n",
    "#     y_fold_pred = np.argmax(predictions, axis=1)\n",
    "#     y_fold_true = np.argmax(y_fold_val, axis=1)\n",
    "\n",
    "#     # Calculate performance metrics\n",
    "#     accuracy = accuracy_score(y_fold_true, y_fold_pred)\n",
    "#     precision = precision_score(y_fold_true, y_fold_pred, average='macro')\n",
    "#     recall = recall_score(y_fold_true, y_fold_pred, average='macro')\n",
    "#     f1 = f1_score(y_fold_true, y_fold_pred, average='macro')\n",
    "\n",
    "#     # Store the performance metrics for the current fold\n",
    "#     accuracies.append(accuracy)\n",
    "#     precisions.append(precision)\n",
    "#     recalls.append(recall)\n",
    "#     f1_scores.append(f1)\n",
    "\n",
    "# # Calculate the mean and standard deviation of the performance metrics across all folds\n",
    "# mean_accuracy = np.mean(accuracies)\n",
    "# std_accuracy = np.std(accuracies)\n",
    "# mean_precision = np.mean(precisions)\n",
    "# std_precision = np.std(precisions)\n",
    "# mean_recall = np.mean(recalls)\n",
    "# std_recall = np.std(recalls)\n",
    "# mean_f1 = np.mean(f1_scores)\n",
    "# std_f1 = np.std(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print performance metrics for the model\n",
    "# print(\"Mean Accuracy of model: {:.2f}%\".format(mean_accuracy * 100))\n",
    "# print(\"Mean Precision of model: {:.2f}%\".format(mean_precision * 100))\n",
    "# print(\"Mean Recall of model: {:.2f}%\".format(mean_recall * 100))\n",
    "# print(\"Mean F1-Score of model: {:.2f}%\".format(mean_f1 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 155ms/step - accuracy: 0.3139 - loss: 2.9312 - val_accuracy: 0.0714 - val_loss: 4.2578 - learning_rate: 5.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 156ms/step - accuracy: 0.7779 - loss: 1.1625 - val_accuracy: 0.2562 - val_loss: 3.4854 - learning_rate: 5.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 156ms/step - accuracy: 0.8831 - loss: 0.8081 - val_accuracy: 0.6356 - val_loss: 1.4922 - learning_rate: 5.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 156ms/step - accuracy: 0.9332 - loss: 0.6459 - val_accuracy: 0.9838 - val_loss: 0.5000 - learning_rate: 5.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 155ms/step - accuracy: 0.9543 - loss: 0.5357 - val_accuracy: 0.9798 - val_loss: 0.4578 - learning_rate: 5.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 162ms/step - accuracy: 0.9677 - loss: 0.4661 - val_accuracy: 0.9576 - val_loss: 0.4590 - learning_rate: 5.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 161ms/step - accuracy: 0.9717 - loss: 0.4132 - val_accuracy: 0.9858 - val_loss: 0.3546 - learning_rate: 5.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 172ms/step - accuracy: 0.9823 - loss: 0.3575 - val_accuracy: 0.9965 - val_loss: 0.2938 - learning_rate: 5.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 178ms/step - accuracy: 0.9828 - loss: 0.3244 - val_accuracy: 0.9978 - val_loss: 0.2620 - learning_rate: 5.0000e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 209ms/step - accuracy: 0.9831 - loss: 0.2957 - val_accuracy: 0.9758 - val_loss: 0.2941 - learning_rate: 5.0000e-04\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 173ms/step - accuracy: 0.2907 - loss: 3.0254 - val_accuracy: 0.0798 - val_loss: 4.3491 - learning_rate: 5.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m 21/172\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 162ms/step - accuracy: 0.6953 - loss: 1.4101"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 69\u001b[39m\n\u001b[32m     66\u001b[39m fold_model.compile(optimizer=optimizer, loss=\u001b[33m'\u001b[39m\u001b[33mcategorical_crossentropy\u001b[39m\u001b[33m'\u001b[39m, metrics=[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Train the model on the training fold data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[43mfold_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatagen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_fold_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_fold_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m           \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m           \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_fold_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_fold_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m           \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlearning_rate_reduction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Evaluate the model on the validation set for the current fold\u001b[39;00m\n\u001b[32m     75\u001b[39m predictions = fold_model.predict(x_fold_val)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:219\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    221\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1681\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1682\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1683\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1684\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1685\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1686\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1687\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1688\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1691\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1692\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1693\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1697\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1698\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Importing necessary library\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the number of splits for K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store performance metrics\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "# Define data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Define learning rate reduction callback with increased patience\n",
    "learning_rate_reduction = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    patience=3,  # Increased from 2 to 3\n",
    "    verbose=1,\n",
    "    factor=0.5,\n",
    "    min_lr=0.00001\n",
    ")\n",
    "\n",
    "# Iterate through each fold\n",
    "for train_index, val_index in kf.split(x_train, y_train):\n",
    "    # Split the data into training and validation sets based on current fold\n",
    "    x_fold_train, x_fold_val = x_train[train_index], x_train[val_index]\n",
    "    y_fold_train, y_fold_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "    # Reinitialize the model for each fold with L2 regularization\n",
    "    fold_model = Sequential()\n",
    "    fold_model.add(Conv2D(75, (3,3), strides=1, padding='same', activation='relu', \n",
    "                         kernel_regularizer=l2(0.001), input_shape=(28,28,1)))\n",
    "    fold_model.add(BatchNormalization())\n",
    "    fold_model.add(MaxPool2D((2,2), strides=2, padding='same'))\n",
    "    fold_model.add(Conv2D(50, (3,3), strides=1, padding='same', activation='relu',\n",
    "                         kernel_regularizer=l2(0.001)))\n",
    "    fold_model.add(Dropout(0.3))  # Increased from 0.2\n",
    "    fold_model.add(BatchNormalization())\n",
    "    fold_model.add(MaxPool2D((2,2), strides=2, padding='same'))\n",
    "    fold_model.add(Conv2D(25, (3,3), strides=1, padding='same', activation='relu',\n",
    "                         kernel_regularizer=l2(0.001)))\n",
    "    fold_model.add(BatchNormalization())\n",
    "    fold_model.add(MaxPool2D((2,2), strides=2, padding='same'))\n",
    "    fold_model.add(Flatten())\n",
    "    fold_model.add(Dense(512, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    fold_model.add(Dropout(0.4))  # Increased from 0.3\n",
    "    fold_model.add(Dense(24, activation='softmax'))\n",
    "    \n",
    "    # Compile with lower initial learning rate\n",
    "    optimizer = Adam(learning_rate=0.0005)  # Lower initial learning rate\n",
    "    fold_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model on the training fold data\n",
    "    fold_model.fit(datagen.flow(x_fold_train, y_fold_train, batch_size=128),\n",
    "               epochs=10,\n",
    "               validation_data=(x_fold_val, y_fold_val),\n",
    "               callbacks=[learning_rate_reduction])\n",
    "\n",
    "    # Evaluate the model on the validation set for the current fold\n",
    "    predictions = fold_model.predict(x_fold_val)\n",
    "    y_fold_pred = np.argmax(predictions, axis=1)\n",
    "    y_fold_true = np.argmax(y_fold_val, axis=1)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(y_fold_true, y_fold_pred)\n",
    "    precision = precision_score(y_fold_true, y_fold_pred, average='macro')\n",
    "    recall = recall_score(y_fold_true, y_fold_pred, average='macro')\n",
    "    f1 = f1_score(y_fold_true, y_fold_pred, average='macro')\n",
    "\n",
    "    # Store the performance metrics for the current fold\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate the mean and standard deviation of the performance metrics across all folds\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "std_accuracy = np.std(accuracies)\n",
    "mean_precision = np.mean(precisions)\n",
    "std_precision = np.std(precisions)\n",
    "mean_recall = np.mean(recalls)\n",
    "std_recall = np.std(recalls)\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "std_f1 = np.std(f1_scores)\n",
    "\n",
    "# Print performance metrics for the model\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(\"Mean Accuracy: {:.2f}% (±{:.2f}%)\".format(mean_accuracy * 100, std_accuracy * 100))\n",
    "print(\"Mean Precision: {:.2f}% (±{:.2f}%)\".format(mean_precision * 100, std_precision * 100))\n",
    "print(\"Mean Recall: {:.2f}% (±{:.2f}%)\".format(mean_recall * 100, std_recall * 100))\n",
    "print(\"Mean F1-Score: {:.2f}% (±{:.2f}%)\".format(mean_f1 * 100, std_f1 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving the best model from fold 1 with accuracy: 97.58%\n",
      "Epoch 1/10\n",
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 178ms/step - accuracy: 0.3312 - loss: 2.8577 - val_accuracy: 0.0485 - val_loss: 4.5242 - learning_rate: 5.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 160ms/step - accuracy: 0.8130 - loss: 1.0561 - val_accuracy: 0.1762 - val_loss: 3.6598 - learning_rate: 5.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 157ms/step - accuracy: 0.9105 - loss: 0.7333 - val_accuracy: 0.9313 - val_loss: 0.7022 - learning_rate: 5.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 157ms/step - accuracy: 0.9474 - loss: 0.5809 - val_accuracy: 0.9389 - val_loss: 0.5599 - learning_rate: 5.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 161ms/step - accuracy: 0.9656 - loss: 0.4836 - val_accuracy: 0.9921 - val_loss: 0.3874 - learning_rate: 5.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 157ms/step - accuracy: 0.9756 - loss: 0.4135 - val_accuracy: 0.9851 - val_loss: 0.3493 - learning_rate: 5.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 166ms/step - accuracy: 0.9811 - loss: 0.3593 - val_accuracy: 0.9915 - val_loss: 0.3154 - learning_rate: 5.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 156ms/step - accuracy: 0.9800 - loss: 0.3228 - val_accuracy: 0.9930 - val_loss: 0.2661 - learning_rate: 5.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 169ms/step - accuracy: 0.9852 - loss: 0.2780 - val_accuracy: 0.9580 - val_loss: 0.3166 - learning_rate: 5.0000e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 161ms/step - accuracy: 0.9873 - loss: 0.2454 - val_accuracy: 0.9967 - val_loss: 0.2144 - learning_rate: 5.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully as 'sign_language_model_v2.h5'\n"
     ]
    }
   ],
   "source": [
    "# Save the best performing model\n",
    "best_fold_idx = np.argmax(accuracies)\n",
    "print(f\"\\nSaving the best model from fold {best_fold_idx + 1} with accuracy: {accuracies[best_fold_idx]*100:.2f}%\")\n",
    "\n",
    "# Recreate the best model\n",
    "best_model = Sequential()\n",
    "best_model.add(Conv2D(75, (3,3), strides=1, padding='same', activation='relu', \n",
    "                     kernel_regularizer=l2(0.001), input_shape=(28,28,1)))\n",
    "best_model.add(BatchNormalization())\n",
    "best_model.add(MaxPool2D((2,2), strides=2, padding='same'))\n",
    "best_model.add(Conv2D(50, (3,3), strides=1, padding='same', activation='relu',\n",
    "                     kernel_regularizer=l2(0.001)))\n",
    "best_model.add(Dropout(0.3))\n",
    "best_model.add(BatchNormalization())\n",
    "best_model.add(MaxPool2D((2,2), strides=2, padding='same'))\n",
    "best_model.add(Conv2D(25, (3,3), strides=1, padding='same', activation='relu',\n",
    "                     kernel_regularizer=l2(0.001)))\n",
    "best_model.add(BatchNormalization())\n",
    "best_model.add(MaxPool2D((2,2), strides=2, padding='same'))\n",
    "best_model.add(Flatten())\n",
    "best_model.add(Dense(512, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "best_model.add(Dropout(0.4))\n",
    "best_model.add(Dense(24, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.0005)\n",
    "best_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the best model on the entire training set\n",
    "best_model.fit(datagen.flow(x_train, y_train, batch_size=128),\n",
    "               epochs=10,\n",
    "               validation_data=(x_test, y_test),\n",
    "               callbacks=[learning_rate_reduction])\n",
    "\n",
    "# Save the model\n",
    "best_model.save('sign_language_model_v2.h5')\n",
    "print(\"Model saved successfully as 'sign_language_model_v2.h5'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
